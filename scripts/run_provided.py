#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
A script that satisfies task 1 (The overview)

This script is based on `reference/bjorklund_extracts.py`.
"""

# Copyright (c) 2025 John Isaac Calderon

# Necessary modules for parsing and general analysis
import matplotlib.pyplot as plt
import numpy as np
import scipy
import h5py

# OS-related hooks
import os
import sys

# For datetime objects
from matplotlib import dates
from datetime import datetime, timedelta
import dateutil.parser

# For map
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from geopy.distance import great_circle

# Modules to speed up calculations
import multiprocessing

# Type hints
from typing import Optional

import time

# Set path for data
cwd = os.path.abspath("") + "/"
dat_path = cwd + "seismic_data/"
fn_list = os.listdir(dat_path)

# Obtain logical cores count
logical_cores = os.cpu_count()

# - calculate number of free cores
reserved_cores = 2
free_cores = logical_cores - reserved_cores

# Calculate worker count
workers_per_core = 2


# Inner routine for `parse`
def _parse(attrs: (str, int)):
    """
    Inner routine for `parse`

    Accepts:
        attrs: (str, int)
            attrs[0]: str - File to be read
            attrs[1]: int - Number of samples to be read from each file

    Returns:
        data: np.ndarray
            Collection of data points
        times: np.ndarray
            Collection of time stamps
        lats: np.ndarray
            Collection of station latitudes
        lons: np.ndarray
            Collection of station longitudes
        dt: (?)
            Collection of time steps
    """

    # Obtain file name
    fname = attrs[0]

    # Obtain sample count
    n_samples = attrs[1]

    # Read from provided file
    raw_data = h5py.File(fname, "r")

    # Read the latitude and longitude attributes for the station and put into vectors
    lats = np.array(raw_data.attrs["latitude"])
    lons = np.array(raw_data.attrs["longitude"])

    # Read tree structure and load dataset into aggregate array
    level1 = list(raw_data.keys())  # /waveforms
    level2 = list(raw_data[level1[0]].keys())  # /waveforms/dataset_name
    dataset = raw_data["/" + level1[0] + "/" + level2[0]]  # Reading dataset in subgroup

    data = dataset[:]  # Extract numpy array from dataset object

    # If missing data, pad with zeros
    if len(data) < n_samples:
        data = np.pad(data, (0, n_samples - len(data)), "constant", constant_values=0)

    # Figure out the start time and generate a time vector
    start_time_str = dataset.attrs["starttime"]
    start_time = dateutil.parser.isoparse(
        start_time_str
    )  # Time attribute on ISO-format

    dt = dataset.attrs["delta"]
    deltas = np.linspace(
        0, dt * (n_samples - 1), n_samples
    )  # A duration which can be added to a datetime object
    times = start_time + deltas * timedelta(seconds=1)

    return data, times, lats, lons, dt


def _unpack(
    result,
    n_files: int,
    n_samples: int,
):
    """
    Unpack routine for `parse`

    Accepts:
        results: list
            List of results (generated by `map(...)`)
        n_files: int
            Number of files
        n_samples:
            Number of samples per file

    Returns:
        data_collection: np.ndarray
            Collection of data points
        times_collection: np.ndarray
            Collection of time stamps
        lats: np.ndarray
            Collection of station latitudes
        lons: np.ndarray
            Collection of station longitudes
        dt: np.ndarray
            Collection of time steps
    """

    # Initialize grid arrays
    data_collection = np.zeros((N_files, N_samples))  # Station data
    times_collection = np.zeros((N_files, N_samples), dtype=datetime)  # Time stamps

    # Initialize flat arrays
    lats = np.zeros(n_files)
    lons = np.zeros(n_files)
    dt = np.zeros(n_files)

    # Unpack serially
    for ii, r in enumerate(result):
        r_data, r_times, r_lats, r_lons, r_dt = r

        # - store the results in their corresponding arrays
        data_collection[ii, :] = r_data  # aggregate data
        times_collection[ii, :] = r_times  # aggregate time

        lats[ii] = r_lats  # aggregate latitude
        lons[ii] = r_lons  # aggregate longitude
        dt[ii] = r_dt  # aggregate timestep

    return data_collection, times_collection, lats, lons, dt


# Read files and store contents. Accepts user input for number of files to read.
def parse(num_files=None):
    """
    Reads files and stores their contents

    Accepts:
        num_files (default: None)
            Number of files to read

    Returns:
        data_collection: np.ndarray
            Collection of data points
        times_collection: np.ndarray
            Collection of time stamps
        lats: np.ndarray
            Collection of station latitudes
        lons: np.ndarray
            Collection of station longitudes
        dt: np.ndarray
            Collection of time steps
    """

    if num_files:
        N_files = num_files
    else:
        N_files = len(fn_list)

    # Size of the datasets in the files. Hard coded unless you want dynamic allocation (lists, slower)
    N_samples = 720000

    # - read files in parallel
    print(
        " I: Will read %d files in parallel across %d cores (%d workers per core)"
        % (N_files, free_cores, workers_per_core)
    )

    with multiprocessing.Pool(free_cores) as p:
        p_attrs = [(dat_path + fn, N_samples) for fn in fn_list]

        t1 = time.time()
        result = p.map(_parse, p_attrs, chunksize=workers_per_core)
        t2 = time.time()

        print(
            " I: (vector operation took %.3f seconds; %.3f seconds per file)"
            % (t2 - t1, (t2 - t1) / N_files)
        )

    # Unpack the result
    t3 = time.time()
    r = _unpack(result, data_collection, times_collection, N_files, N_samples)
    t4 = time.time()

    print(" I: (unpacking took %.3f seconds)" % (t4 - t3))

    print("done")
    return r


def plot_map(lats, lons, tonga_latlon):
    northward_offset = 90  # As in sample program
    central_lat = tonga_latlon[0] + northward_offset
    central_lon = tonga_latlon[1]

    fig = plt.figure(figsize=(15, 15))
    ax = fig.add_subplot(
        1,
        1,
        1,
        projection=ccrs.AzimuthalEquidistant(
            central_latitude=central_lat, central_longitude=central_lon
        ),
    )

    # Stock background (Natural Earth)
    ax.stock_img()

    # Add features on top of stock_img for better resolution
    ax.add_feature(cfeature.LAND, alpha=0.5, zorder=1)
    ax.add_feature(cfeature.COASTLINE, linewidth=0.3, zorder=1)
    ax.add_feature(cfeature.BORDERS, linestyle=":", linewidth=0.3, alpha=1, zorder=1)

    # Make the map global rather than have it zoom in to the extents of any plotted data
    ax.set_global()

    # Make gridlines
    ax.gridlines(crs=ccrs.PlateCarree())

    # Plot stations and Hunga Tonga
    ax.scatter(
        tonga_latlon[1],
        tonga_latlon[0],
        marker="x",
        linewidth=2,
        color="red",
        transform=ccrs.PlateCarree(),
    )  # Transform is PlateCarree regardless of projection

    ax.scatter(
        lons,
        lats,
        marker="^",
        linewidth=0.5,
        facecolor="none",
        edgecolor="magenta",
        transform=ccrs.PlateCarree(),
    )  # Transform is PlateCarree regardless of projection

    plt.show()


def circle_distance(n_files, lats, lons, tonga_latlon):
    # Calculate great circle distance. Using geopy.great_circle
    dists = []
    for i in range(n_files):
        dists.append(
            great_circle((tonga_latlon[0], tonga_latlon[1]), (lats[i], lons[i])).m
        )

    dists = np.array(dists)
    dists_km = dists / 1000

    print()
    print(
        " I: Smallest great cricle distance is {:.2f} km. Largest great circle distance is {:.2f} km".format(
            np.min(dists_km), np.max(dists_km)
        )
    )

    plt.style.use("seaborn-whitegrid")
    n_vec = np.linspace(0, n_files - 1, n_files)
    fig = plt.figure(figsize=(15, 10))
    ax = fig.add_subplot(111)
    ax.scatter(n_vec, np.sort(dists_km), s=0.5)
    ax.set_xlabel("Station number", fontsize=28)
    ax.set_ylabel("Distance, [km]", fontsize=28)
    ax.set_title("Great circle distances between Hunga Tonga and stations", fontsize=28)
    plt.show()

    return dists_km


def __main():
    print(
        " I: Found %d logical cores (%d reserved, %d free)"
        % (logical_cores, reserved_cores, free_cores)
    )

    # READ DATA
    data_collection, times_collection, lats, lons, dt = parse()
    n_files = len(lats)

    # Hunga Tonga location
    tonga_latlon = [-20.550, -175.385]  # latitude and longitude

    # Map
    plot_map(lats, lons, tonga_latlon)

    dists_km = circle_distance(n_files, lats, lons, tonga_latlon)


if __name__ == "__main__":
    __main()
