#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
A script that satisfies task 1 (The overview)

This script is based on `reference/bjorklund_extracts.py`.
"""

# Copyright (c) 2025 John Isaac Calderon

# Necessary modules for parsing and general analysis
import matplotlib.pyplot as plt
import matplotlib.figure
import numpy as np
import scipy
import h5py

# OS-related hooks
import os
import sys

# For datetime objects
from matplotlib import dates
from datetime import datetime, timedelta
import dateutil.parser

# For map
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from geopy.distance import great_circle

# Modules to speed up calculations
import multiprocessing

# Type hints
from typing import Optional, Tuple

# Module to measure elapsed time
import time

# Use custom definitions
from . import is_older

# Set path for data
cwd = os.path.abspath("")
dat_path = os.path.join(cwd, "seismic_data")
cache_path = os.path.join(cwd, "unpacked.h5")

fn_list = os.listdir(dat_path)

# Obtain logical cores count
logical_cores = os.cpu_count()

# - calculate number of free cores
reserved_cores = 2
free_cores = logical_cores - reserved_cores

# Calculate worker count
workers_per_core = 1


# Inner routine for `parse`
def __parse(attrs: (str, int)):
    """
    Inner routine for `parse`

    Parameters
    ----------
    attrs: (str, int)
        attrs[0]: str - File to be read
        attrs[1]: int - Number of samples to be read from each file

    Returns
    -------
    data: np.ndarray
        Collection of data points
    times: np.ndarray
        Collection of time stamps
    lats: np.ndarray
        Collection of station latitudes
    lons: np.ndarray
        Collection of station longitudes
    dt: (?)
        Collection of time steps
    """

    # Obtain file name
    fname = attrs[0]

    # Obtain sample count
    n_samples = attrs[1]

    # Read from provided file
    with h5py.File(fname, "r") as raw_data:
        # Read the latitude and longitude attributes for the station and put into vectors
        lats = np.array(raw_data.attrs["latitude"])
        lons = np.array(raw_data.attrs["longitude"])

        # Read tree structure and load dataset into aggregate array
        level1 = list(raw_data.keys())  # /waveforms
        level2 = list(raw_data[level1[0]].keys())  # /waveforms/dataset_name
        dataset = raw_data[
            "/" + level1[0] + "/" + level2[0]
        ]  # Reading dataset in subgroup

        data = dataset[:].astype(np.float32)  # Extract numpy array from dataset object

        # If missing data, pad with zeros
        if len(data) < n_samples:
            data = np.pad(
                data, (0, n_samples - len(data)), "constant", constant_values=0
            )

        # Figure out the start time and generate a time vector
        start_time_str = dataset.attrs["starttime"]  # - time attribute in ISO 8601
        start_time = np.datetime64(dateutil.parser.isoparse(start_time_str))

        # - convert timestep to nanoseconds
        dt = np.timedelta64(int(dataset.attrs["delta"] * 1e9), "ns")

        # - generate time vector
        times = start_time + np.arange(n_samples) * dt

    return data, times, lats, lons, dt


def __unpack(
    result,
    n_files: int,
    n_samples: int,
):
    """
    Unpack routine for `parse`

    Accepts:
        results: list
            List of results (generated by `map(...)`)
        n_files: int
            Number of files
        n_samples:
            Number of samples per file

    Returns:
        data_collection: np.ndarray
            Collection of data points
        times_collection: np.ndarray
            Collection of time stamps
        lats: np.ndarray
            Collection of station latitudes
        lons: np.ndarray
            Collection of station longitudes
        dt: np.ndarray
            Collection of time steps
    """

    # Initialize grid arrays
    data_collection = np.zeros((n_files, n_samples))  # Station data
    times_collection = np.zeros(
        (n_files, n_samples), dtype="datetime64[ns]"
    )  # Time stamps

    # Initialize flat arrays
    lats = np.zeros(n_files)
    lons = np.zeros(n_files)
    dt = np.zeros(n_files)

    # Unpack "Pythonically"
    r_data, r_times, r_lats, r_lons, r_dt = zip(*result)
    data_collection = np.vstack(r_data)
    times_collection = np.vstack(r_times)
    lats = np.array(r_lats)
    lons = np.array(r_lons)
    dt = np.array(r_dt)

    return data_collection, times_collection, lats, lons, dt


# Read files and store contents. Accepts user input for number of files to read.
def parse(num_files: Optional[int] = None, num_samples: int = 720_000):
    """
    Reads files and stores their contents

    Parameters
    ----------
    num_files : Optional[int] = None
        Number of files to read
    num_samples : int = 720_000
        Number of samples to read from each file

    Returns
    -------
    data_collection: np.ndarray
        Collection of data points
    times_collection: np.ndarray
        Collection of time stamps
    lats: np.ndarray
        Collection of station latitudes
    lons: np.ndarray
        Collection of station longitudes
    dt: np.ndarray
        Collection of time steps
    """

    # Get counts
    if num_files:
        N_files = num_files
    else:
        N_files = len(fn_list)

    # Size of the datasets in the files. Hard coded unless you want dynamic allocation (lists, slower)
    N_samples = num_samples

    # - assume 12 bytes per data point
    in_size = 12 * N_files * N_samples

    # Load from cache
    # - check if `unpacked.h5` is older than all sources
    if all(is_older(os.path.join(dat_path, f), cache_path) for f in fn_list):
        try:
            t0 = time.time()

            with h5py.File("unpacked.h5", "r") as f:
                data_collection = f["data_collection"][:].astype(np.float32)
                unit = f["times_collection"].attrs["unit"]
                times_collection = f["times_collection"][:].view(f"datetime64[{unit}]")

                lats = f["lats"][:]
                lons = f["lons"][:]
                dt = f["dt"][:].view(f"timedelta64[{unit}]")

            t1 = time.time()

            # - calculate elapsed time and data rate
            elapsed = t1 - t0
            rate = in_size / elapsed / (2**30)

            print(f" I: (read took {t1-t0:.3f} seconds; eff. rate: {rate:.3f} GiB/s)")

            # - return only if the cache is "sane"
            return data_collection, times_collection, lats, lons, dt
        except (OSError, KeyError) as e:
            print(f" W: Cache invalid ({e}); rebuilding....")

    # Read files in parallel
    print(
        " I: Will read %d files in parallel across %d cores (%d workers per core)"
        % (N_files, free_cores, workers_per_core)
    )

    with multiprocessing.Pool(free_cores) as p:
        p_attrs = [(os.path.join(dat_path, fn), N_samples) for fn in fn_list]

        t1 = time.time()
        # - use `__parse` backend
        result = p.map(__parse, p_attrs, chunksize=workers_per_core)
        t2 = time.time()

        # - assume 2 * 8 bytes per data point, with 2:1 compression
        in_size = 8 * N_files * N_samples

        print(
            " I: (read took %.3f seconds; eff. rate: %.3f GiB/s)"
            % (t2 - t1, in_size / (t2 - t1) / (2**30))
        )

        # - properly close pool
        p.close()
        p.join()

    # Unpack the result
    t3 = time.time()
    r = __unpack(result, N_files, N_samples)
    t4 = time.time()

    print(" I: (unpacking took %.3f seconds)" % (t4 - t3))

    # - unpack result syntactically
    data_collection, times_collection, lats, lons, dt = r

    # Build cache
    print(" I: Building cache...")

    # - write to temporary file
    with h5py.File(cache_path + ".tmp", "w") as f:
        f.create_dataset(
            "data_collection", data=data_collection, chunks=None, compression=None
        )

        dset = f.create_dataset(
            "times_collection",
            data=times_collection.view("int64"),
            chunks=None,
            compression=None,
        )

        dset.attrs["unit"] = "ns"

        f.create_dataset("lats", data=lats)
        f.create_dataset("lons", data=lons)
        f.create_dataset("dt", data=dt.view("int64"))

        # - flush FS caches
        f.flush()

    # - copy to `.h5` on success
    os.replace(cache_path + ".tmp", cache_path)

    print("done")
    return r


def plot_map(lats, lons, tonga_latlon, show: bool = True):
    northward_offset = 90  # As in sample program
    central_lat = tonga_latlon[0] + northward_offset
    central_lon = tonga_latlon[1]

    fig = plt.figure(figsize=(10, 6))
    ax = fig.add_subplot(
        1,
        1,
        1,
        projection=ccrs.AzimuthalEquidistant(
            central_latitude=central_lat, central_longitude=central_lon
        ),
    )

    # Stock background (Natural Earth)
    ax.stock_img()

    # Add features on top of stock_img for better resolution
    ax.add_feature(cfeature.LAND, alpha=0.5, zorder=1)
    ax.add_feature(cfeature.COASTLINE, linewidth=0.3, zorder=1)
    ax.add_feature(cfeature.BORDERS, linestyle=":", linewidth=0.3, alpha=1, zorder=1)

    # Make the map global rather than have it zoom in to the extents of any plotted data
    ax.set_global()

    # Make gridlines
    ax.gridlines(crs=ccrs.PlateCarree())

    # Plot stations and Hunga Tonga
    ax.scatter(
        tonga_latlon[1],
        tonga_latlon[0],
        marker="x",
        linewidth=2,
        color="red",
        transform=ccrs.PlateCarree(),
    )  # Transform is PlateCarree regardless of projection

    ax.scatter(
        lons,
        lats,
        marker="^",
        linewidth=0.5,
        facecolor="none",
        edgecolor="magenta",
        transform=ccrs.PlateCarree(),
    )  # Transform is PlateCarree regardless of projection

    if show:
        plt.show()
        return None
    else:
        return fig


def circle_distance(n_files, lats, lons, tonga_latlon, show: bool = True):
    # Calculate great circle distance. Using geopy.great_circle
    dists = []
    for i in range(n_files):
        dists.append(
            great_circle((tonga_latlon[0], tonga_latlon[1]), (lats[i], lons[i])).m
        )

    dists_km = np.array(dists) / 1000

    # - Obtain smallest and largest distances
    dist_min, dist_max = np.min(dists_km), np.max(dists_km)

    # - Break output into two lines
    print(f" I: Smallest great cricle distance is {dist_min:.2f} km")
    print(f" I: Largest great circle distance is {dist_max:.2f} km")

    plt.style.use("seaborn-whitegrid")
    n_vec = np.linspace(0, n_files - 1, n_files)
    fig = plt.figure(figsize=(10, 6))
    ax = fig.add_subplot(111)
    ax.scatter(n_vec, np.sort(dists_km), s=0.5)
    ax.set_xlabel("Station number")
    ax.set_ylabel("Distance [km]")
    ax.set_title("Great circle distances between Hunga Tonga and stations")

    if show:
        plt.show()
        return dists_km, None
    else:
        return dists_km, fig


def main(
    show_plots: bool = True,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Main routine for `run_provided`

    The routine reads raw station data, stores them in distinct arrays
    containing seismic data, time stamps, location and time steps.

    Parameters
    ----------
    Does not accept any parameters

    Returns
    -------
    data_collection : np.ndarray
        An array with the shape (S,T), where `S` is the number
        of stations, and `T` is the number of data points
    times_collection : np.ndarray
        An array with the shape (S,T), where `S` is the number
        of stations, and `T` is the number of time stamps

    lats, lons : np.ndarray
        Flat arrays containing the geographical location
        of the stations

    dt : np.ndarray
        A flat array containing the time step for each station
    dists : np.ndarray
        Distances between each station and the event (in km)
    map_fig, dist_fig : Tuple[Optional[matplotlib.figure.Figure]]
        Figures to be shown
    """

    print(
        " I: Found %d logical cores (%d reserved, %d free)"
        % (logical_cores, reserved_cores, free_cores)
    )

    # READ DATA
    data_collection, times_collection, lats, lons, dt = parse()
    n_files = len(lats)

    # Hunga Tonga location
    tonga_latlon = [-20.550, -175.385]  # latitude and longitude

    # Map
    map_fig = plot_map(lats, lons, tonga_latlon, show=show_plots)

    # Obtain great circle distances
    # - the shortest and longest distances are already
    # written to standard output, so I don't exactly
    # know what to do with the returned array
    dists_km, dist_fig = circle_distance(
        n_files, lats, lons, tonga_latlon, show=show_plots
    )

    # Return segregated data
    return (
        data_collection,
        times_collection,
        lats,
        lons,
        dt,
        dists_km,
        (map_fig, dist_fig),
    )


# Disallow direct execution
if __name__ == "__main__":
    print(" E: Direct execution not allowed")
    print("Please use `run_all.py' instead")
    sys.exit(1)
